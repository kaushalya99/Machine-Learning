# -*- coding: utf-8 -*-
"""Fareamount_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nNyhioO4Icmxw6HoNB2mpdQto3gxxUBh
"""

!pip install scikit-learn==1.0.2

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from math import radians, cos, sin, sqrt, atan2

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/uber.csv')

"""# **Data Preprocessing**"""

# Drop irrelevant columns
data.drop(columns=["Unnamed: 0", "key"], inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
data["dropoff_longitude"] = imputer.fit_transform(data[["dropoff_longitude"]])
data["dropoff_latitude"] = imputer.fit_transform(data[["dropoff_latitude"]])

# Extract date-time features (separate date and time components)
data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])

# Extract date components (separate day, month, year, weekday)
data['pickup_day'] = data['pickup_datetime'].dt.day
data['pickup_month'] = data['pickup_datetime'].dt.month
data['pickup_year'] = data['pickup_datetime'].dt.year
data['pickup_weekday'] = data['pickup_datetime'].dt.weekday

# Extract time components (hour, minute)
data['pickup_hour'] = data['pickup_datetime'].dt.hour
data['pickup_minute'] = data['pickup_datetime'].dt.minute

# Drop the original 'pickup_datetime' column since we now have separate date and time features
data.drop(columns=['pickup_datetime'], inplace=True)

# Haversine distance calculation
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Earth radius in km
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

data['distance_km'] = data.apply(lambda row: haversine(
    row['pickup_latitude'], row['pickup_longitude'],
    row['dropoff_latitude'], row['dropoff_longitude']), axis=1)

# Remove outliers
data = data[(data['fare_amount'] > 0) & (data['fare_amount'] < 500)]
data = data[(data['distance_km'] > 0) & (data['distance_km'] < 100)]
data = data[(data['passenger_count'] > 0) & (data['passenger_count'] < 10)]

# Skewness Check and Log Transformation
from scipy.stats import skew
if abs(skew(data['fare_amount'])) > 1:
    data['fare_amount'] = np.log1p(data['fare_amount'])

# Split the data
X = data.drop(columns=['fare_amount'])
y = data['fare_amount']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize Numerical Features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Boosting Process Visualization
learning_rate = 0.1
n_trees = 5  # Use a small number of trees for demonstration
base_prediction = np.mean(y_train)  # Initial prediction is the mean of target
residuals = y_train - base_prediction  # Initial residuals
predictions = [base_prediction]  # Store the intermediate results

# Sequentially add trees
for tree_idx in range(1, n_trees + 1):
    tree = xgb.XGBRegressor(n_estimators=1, learning_rate=learning_rate, max_depth=3, random_state=42)
    tree.fit(X_train, residuals)
    pred_residuals = tree.predict(X_train)
    residuals -= learning_rate * pred_residuals
    updated_predictions = predictions[-1] + learning_rate * pred_residuals
    predictions.append(updated_predictions)

"""Gradient Boosting process after a certain number of trees (in this case, after Tree 5).



At the early stages of boosting (after only a few trees), the model predictions are relatively basic and may not fit the data well. This is why the red line appears flat or uniform.
"""

# Plot the boosting process
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y_train)), y_train, label='True Values', color='blue', alpha=0.6)
plt.plot(range(len(y_train)), updated_predictions, label=f'Prediction after Tree {tree_idx}', color='red')
plt.title(f'Boosting Progress: Tree {tree_idx}')
plt.xlabel('Sample Index')
plt.ylabel('Target (Fare Amount)')
plt.legend()
plt.show()

"""# **Training Full Model**"""

# Train Full XGBoost Model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=3, verbose=2)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

"""# **Model evaluation**"""

# Model Evaluation
y_pred = best_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")

# Feature Importance Plot
xgb.plot_importance(best_model, max_num_features=10)
plt.show()

# Residual Plot
residuals = y_test - y_pred
plt.scatter(y_test, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residuals vs Actual Fare Amount")
plt.xlabel("Actual Fare Amount")
plt.ylabel("Residuals")
plt.show()

# Save the model
joblib.dump(best_model, "uber_fare_model.pkl")

import joblib

# Load the model
model = joblib.load("uber_fare_model.pkl")

# Assuming your input data has the following features:
   # pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, pickup_day, pickup_month, pickup_year, pickup_weekday, pickup_hour, pickup_minute, distance_km
new_data = [[ -73.982155, 40.767937, -73.964630, 40.765602, 5, 14, 6, 2010, 0, 17, 26, 1.4424010880737737]]

   # Create a pandas DataFrame if necessary:
new_data_df = pd.DataFrame(new_data, columns=X.columns)

   # Apply feature scaling if used during training:
new_data_scaled = scaler.transform(new_data_df)

prediction = model.predict(new_data_scaled)
print(prediction)